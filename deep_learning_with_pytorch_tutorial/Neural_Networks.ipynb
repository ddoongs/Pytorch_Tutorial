{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "NN은 torch.nn 패키지를 사용하여 생성할 수 있다.\n",
    "\n",
    "nn은 모델을 정의하고 미분하는데 autograd를 사용한다. nn.Module은 layer와 output을 반환하는 forward(input) 메서드를 포함하고 있다.\n",
    "\n",
    "숫자 이미지를 분류하는 신경망을 예제로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 사진은 간단한 feed-forward network이다. input을 받아 여러 계층에 차례로 전달 후, output을 제공한다.\n",
    "\n",
    "신경망의 일반적인 학습 과정  \n",
    "\n",
    "1. 학습 가능한 매개변수를 갖는 신경망을 정의.\n",
    "2. dataset 입력을 반복\n",
    "3. 입력을 신경망에서 전파\n",
    "4. loss를 계산\n",
    "5. gradient를 신경망의 매개변수들에 역으로 전파\n",
    "6. 신경망의 가중치를 갱신 (new_weight = weight - LR*gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # input image channel = 1\n",
    "        # output channel = 6\n",
    "        # 5*5 convolutional matrix\n",
    "        \n",
    "        # Convolution kernel 정의\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        # Affine operation : y =Wx + b\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (2, 2) 크기 window에 대해 max_pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # 크기가 제곱수라면, 하나의 숫자만을 specify(특정)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # 배치 차원을 제외한 모든 차원을 하나로 flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward 함수만 정의하고 나면 gradient를 계산하는 backward 함수는 autograd를 사용하여 자동으로 정의된다. forward 함수에서는 어떠한 tensor 연산을 사용해도 된다.\n",
    "\n",
    "모델의 학습 가능한 매개변수들은 net.parameters()에 의해 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # conv1의 weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 32*32 입력을 넣어보자.\n",
    "\n",
    "MNIST를 쓰기 위해서는 dataset의 image size를 32*32로 바꿔야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.4480e-02, -1.4264e-01,  1.7299e-02,  1.1021e-01,  5.6953e-02,\n",
      "         -1.3981e-05, -5.6526e-02, -2.4025e-02,  4.1463e-02, -6.5780e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 매개변수의 gradient buffer를 0으로 설정하고, 무작위 값으로 back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn은 mini-batch만 지원한다.  \n",
    "torch.nn 패키지 전체는 하나의 샘플이 아닌, 샘플들의 미니배치만을 입력으로 받는다.\n",
    "\n",
    "예를 들어, nnConv2D는 nSamples* nChannels* height*width의 4차원 tensor를 입력으로 한다.  \n",
    "만약 하나의 샘플만 있다면, input.unsqueeze(0)을 사용해서 가상의 차원을 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약\n",
    "\n",
    "1. torch.Tensor : backward() 같은 autograd 연산을 지원하는 다차원 배열\n",
    "2. nn.Module : 신경망 모듈. 매개변수를 캡슐화하는 간편한 방법\n",
    "3. nn.Parameter : Tensor의 한 종류, Module에 속성으로 할당될 때 자동으로 매개변수로 등록\n",
    "4. autograd.Function : autograd 연산의 순방향과 역방향 정의를 구현."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*지금까지 다룬 내용*\n",
    "- 신경망 정의\n",
    "- 입력을 처리하고 backward 호출하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "loss function은 (output, target)을 한 쌍으로 입력 받아, output이 target으로부터 얼마나 멀리 떨어져 있는지를 추정하는 값을 계산.\n",
    "\n",
    "nn 패키지에는 여러 loss function이 존재. 간단한 loss function으로는 출력과 대상간의 mean-squared error를 계산하는 nn.MSEloss가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1723, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10) #예시를 위한 임의의 정답\n",
    "target = target.view(1, -1) # 출력과 같은 shape으로 만듦\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 .grad_fn 속성을 이용해 loss를 역방향에서 따라가다보면, 이러한 모습의 연산 그래프를 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> flatten -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서, loss.backward()를 실행할 때, 전체 그래프는 신경망의 매개변수에 의해 미분되며, 그래프 내의 requires_grad=True 인 모든 Tensor는 gradient가 누적된 .grad tensor를 갖게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x10f3ee280>\n",
      "<AddmmBackward0 object at 0x10f3ee820>\n",
      "<AccumulateGrad object at 0x10f3ee280>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0]) # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop\n",
    "\n",
    "error를 역전파하기 위해서는 loss.backward()만 해주면 된다. 기존에 계산된 gradient 값을 누적 시키고 싶지 않다면 기존에 계산된 gradient를 0으로 만드는 작업이 필요하다.\n",
    "\n",
    "loss.backward()를 호출하여 역전파 전과 후에 conv1의 bias 변수의 gradient를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0183,  0.0040,  0.0171, -0.0044, -0.0229,  0.0055])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad() # 모든 매개변수의 변화도 버퍼를 0으로 만듦\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가중치 갱신\n",
    "\n",
    "실제로 많이 사용되는 가장 단순한 갱신 규칙은 Stochastic Gradient Descent 이다.\n",
    "\n",
    "새로운 가중치(weight) = 가중치(weight) - 학습률(learning rate) * 변화도(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data*learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 구성할 때, SGD, Adam, RMSProp 등과 같은 다양한 optimizer를 사용하고 싶을 수 있다.\n",
    "\n",
    "이를 위해서 torch.optim 라는 작은 패키지에 이것들을 모두 구현해 두었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer를 생성한다.\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# 학습과정은 다음과 같다.\n",
    "optimizer.zero_grad() #누적된 gradient 초기화\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step() # 업데이트 진행"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704802e396c47ec2fbe49fdd63907b8569bda7598820a76a954d72ad341bac6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
