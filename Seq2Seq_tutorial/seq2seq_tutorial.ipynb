{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN.TRANSFORMER 와 TORCHTEXT 로 시퀀스-투-시퀀스(SEQUENCE-TO-SEQUENCE) 모델링하기\n",
    "\n",
    "이 튜토리얼에서는 nn.Transformer 모듈을 이용하는 Seq2Seq 모델을 학습하는 방법을 배워보겠습니다.\n",
    "\n",
    "PyTorch 1.2 버젼에는 Attention is All You Need 논문에 기반한 표준 트랜스포머(transformer) 모듈을 포함하고 있습니다. 트랜스포머 모델은 다양한 seq2seq 문제들에서 더 병렬화가 가능하면서 RNN과 비교하여 더 나은 성능을 보임이 입증되었다. \n",
    "\n",
    "nn.Transformer 모듈은 입력과 출력 사이의 전역적인 의존성을 나타내기 위해 nn.MultiheadAttention에 전적으로 의존한다. \n",
    "\n",
    "현재 nn.Transformer 모듈은 모듈화가 잘 되어 있어 단일 컴포넌트로 쉽게 적용 및 구성할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/Users/junghwankim/Desktop/pytorch_tutorial/Seq2Seq_tutorial/transformer_architecture.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 정의하기\n",
    "\n",
    "이 튜토리얼에서 우리는 nn.TransformerEncoder 모델을 Language modeling 과제에 대해서 학습시킬 것이다. 언어 모델링 과제는 주어진 단어가 다음에 이어지는 단어 시퀀스를 따를 likelihood에 대한 확률을 할당하는 것이다. \n",
    "\n",
    "먼저, 토큰들의 시퀀스가 임베딩 레이어로 전달되며, 이어서 포지셔널 인코딩 레이어가 각 단어의 순서를 설명한다. \n",
    "\n",
    "nn.TransformerEncoder는 여러 개의 nn.TransformerEncoderLayer로 구성되어 있다. nn.TransformerEncoder 내부의 셀프-어텐션 레이어들은 시퀀스 안에서의 이전 포지션에만 집중하도록 허용되기 때문에 입력순서와 함께 정사각 형태의 어텐션 마스크가 필요하다. 언어 모델링 과제를 위해 미래의 포지션에 있는 모든 토큰들은 마스킹 되어 가려져야 한다. 실제 단어를 얻기 위해, nn.TransformerEncoder의 출력은 로그-소프트맥스로 이어지는 최종 선형 레이어로 전달된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple \n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, src : Tensor, src_mask: Tensor -> Tensor):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704802e396c47ec2fbe49fdd63907b8569bda7598820a76a954d72ad341bac6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
